{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 9: (Single-Layer) Neural Networks and Gradient Descent (50 Pts)\n",
    "\n",
    "In this lab, we are going to implement gradient descent to train a simple (single-layer) neural network. We will use the [Autograd](https://github.com/HIPS/autograd) library to easily compute the gradients necessary to solve a toy learning problem.\n",
    "\n",
    "\n",
    "Before we begin, we will first install the Autograd. The simplest way to do this is to run the following command in your terminal after activating the `mae345` environment.\n",
    "\n",
    "`conda install -c conda-forge autograd`\n",
    "\n",
    "You may need to restart Jupyter after running the command.\n",
    "\n",
    "The main functionality Autograd offers is the ability to automatically compute gradients (hence the name!) for complicated mathematical functions. This saves a lot of time instead of having to repeatedly perform the chain rule or relying on numerical differentiation, which can be slow for functions of many variables. You can write mathematical functions using Numpy syntax and Autograd will allow  you to compute gradients. As an example, take a look at the block of code below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autograd import grad\n",
    "import autograd.numpy as np\n",
    "\n",
    "# Define a function using numpy operations \n",
    "def my_function(w,x,y):\n",
    "    return np.power(w,2)*x + y\n",
    "\n",
    "# Gradient function (partial derivative of my_function w.r.t. the first argument, i.e., w)\n",
    "gradient_fun = grad(my_function) \n",
    "\n",
    "# Evaluate the gradient (partial derivative) of my_function with respect to w at w=w0, x=x0, y=y0\n",
    "w0 = 0.5\n",
    "x0 = 0.2\n",
    "y0 = 0.3\n",
    "print(gradient_fun(w0, x0, y0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can manually verify that `gradient_fun(x0, x0, y0)` evaluates the gradient of `my_function` w.r.t. the first argument of `my_function` (which is `w`) at `w0, x0, y0`. The beauty of Autograd is that `my_function` can be extremely complicated; manually computing the gradient might be very annoying, but Autograd saves you the trouble. \n",
    "\n",
    "Notice also above that we are not actually using numpy above. We are using autograd.numpy as if it were numpy. Autograd has overloaded a large number of numpy operations. As long as `my_function` uses these basic numpy operations, Autograd will allow you to compute gradients. You can take a look at the Autograd documentation for all the numpy functions Autograd has overloaded (but we won't need anything fancy for this assignment --- you can just pretend like autograd.numpy is numpy)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will train a single-layer neural network using gradient descent. Your task is to fill in the portions marked \"TODO\". \n",
    "\n",
    "First, let's define some helper functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sigmoid nonlinearity (i.e., activation)\n",
    "def sigmoid(z):\n",
    "    return 0.5 * (np.tanh(z / 2.) + 1)\n",
    "\n",
    "######## TODO: Fill in these functions ################################\n",
    "\n",
    "# Single-layer neural network (with no bias) \n",
    "def neural_network_prediction(weights, x):\n",
    "    '''This function should implement a single-layer neural network. We will\n",
    "    ignore the bias term here to keep things simple. This function\n",
    "    should take in an input (x) and output sigmoid(w'*x). Recall that\n",
    "    matrix multiplication in numpy can be done using np.dot().'''\n",
    "    return\n",
    "\n",
    "# Loss function: Binary Cross Entropy \n",
    "def loss_function(y_pred, y_true):\n",
    "    '''Loss function: this function takes in a predicted label (scalar)\n",
    "    and a true label (scalar) and outputs a scalar that quantifies how\n",
    "    good the prediction is. Implement the binary cross entropy loss\n",
    "    discussed in Lecture 20.'''\n",
    "    return\n",
    "\n",
    "# Training loss\n",
    "def training_loss(weights, inputs, labels):\n",
    "    '''Compute the total training loss here (i.e., the loss summed\n",
    "    over all the inputs in the dataset)''' \n",
    "    return\n",
    "\n",
    "#######################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will define some training data (inputs (x) and labels (y))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############ DO NOT MODIFY ############################################\n",
    "# Build a toy dataset\n",
    "inputs = np.array([[1.0,  0.0, 0.25, -1.0,  0.0, -0.25],\n",
    "                   [0.0,  1.0, 0.3,   0.0, -1.0, -0.4]])\n",
    "\n",
    "labels = np.array([1, 1, 1, 0, 0, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we will optimize our weights to minimize the training loss using gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############ DO NOT MODIFY #############################################\n",
    "# Define a compiled function that returns gradients of the training loss.\n",
    "# This is identical to the simple gradient example we considered at the\n",
    "# very beginning of this notebook. The \"jit\" command does \"just-in-time\n",
    "# compilation\". Don't worry too much about what jit is. This helps speed\n",
    "# up the code. \n",
    "training_gradient_fun = grad(training_loss)\n",
    "########################################################################\n",
    "\n",
    "# Optimize weights using gradient descent\n",
    "weights = np.ones(2) # Initialize the weights to the all-ones vector (this doesn't really matter.Random\n",
    "# initializations are ok too.)\n",
    "# Print the training loss with the initial weights\n",
    "print(\"Initial loss:\", training_loss(weights, inputs, labels))\n",
    "\n",
    "############### TODO: Fill in code to do gradient descent ##############\n",
    "step_size = 0.1\n",
    "# Take gradient steps\n",
    "for i in range(100):\n",
    "    pass\n",
    "\n",
    "\n",
    "# Print the training loss with the optimized weights    \n",
    "print(\"Trained loss:\", training_loss(weights, inputs, labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the block below visualizes the outputs of the trained neural network. Note that the true labels are either 1 (plus) or 0 (dot) and the output of your single-layer neural network will also be between 0 and 1. The contour plot below shows level-sets of the neural network's predictions. The level set corresponding to 0.5 is the \"decision boundary\". If trained correctly, the 0.5-level set should separate the points with label equals to 1 from the points with labels equal to 0.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############ DO NOT MODIFY #############################################\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as npp\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "plt.scatter(inputs[0,0:3], inputs[1,0:3], c='r',marker='+')\n",
    "plt.scatter(inputs[0,3:6], inputs[1,3:6], c='b',marker='.')\n",
    "\n",
    "delta = 0.2\n",
    "xs = np.arange(-1.5, 1.5, delta)\n",
    "ys = np.arange(-1.5, 1.5, delta)\n",
    "X, Y = np.meshgrid(xs, ys)\n",
    "\n",
    "Z = npp.zeros(np.shape(X))\n",
    "for i in range(0,len(xs)):\n",
    "    for j in range(0,len(xs)):\n",
    "        xi = np.transpose(np.array([X[i,j], Y[i,j]]))\n",
    "        Z[i,j] = neural_network_prediction(weights, xi)\n",
    "\n",
    "\n",
    "CS = ax.contour(X, Y, Z) \n",
    "\n",
    "ax.clabel(CS, inline=1, fontsize=10)\n",
    "ax.set_title('Neural network predictions')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission Instructions\n",
    "\n",
    "Please submit to Gradescope \"HW8: Coding\" a zip including: this notebook Lab9 (50pts) along with Lab8 notebook and its video (50pts).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
