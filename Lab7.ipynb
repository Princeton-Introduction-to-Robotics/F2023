{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 7: Optical Flow (40 Pts)\n",
    "\n",
    "In this lab, we are going to implement the optical flow algorithm we discussed in class. Optical flow is a very useful algorithm for a wide variety of robotics tasks, including state estimation, tracking objects, and dynamically avoiding obstacles. It's also very straightforward to implement. \n",
    "\n",
    "Before we begin, you need to install FFMPEG to run this assignment. To do so, run `sudo apt install ffmpeg` (Linux) or `brew install ffmpeg` (Mac) in your terminal. Instructions for doing so may be found in previous labs (e.g. Lab 2). You may also need to install opencv using the following command in Anaconda Prompt (be sure the MAE345 environment is activated): `pip install opencv-python`\n",
    "\n",
    "Also, you should be aware that a couple of the cells in this notebook render videos. While designing the assignment, the instructors found that Jupyter continues to store previous videos in memory if you rerun the cell multiple times. As a result, your computer may run out of RAM if you run the cell too many times. If this does occur, you can kill the Jupyter process in the terminal by pulling up the terminal window running Jupyter and hitting `Ctrl+C`.\n",
    "\n",
    "In this lab, you will record a video with your crazyflie and run optical flow on this video.\n",
    "\n",
    "Let's start by testing the camera module.\n",
    "\n",
    "**Please note: You should work with your team to setup the camera/receiver and record the video, but you must complete the optical flow portion of the assignment individually. So, every team member should submit their own completed notebook along with the two video files produced.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instructions for adding camera module for crazyflie\n",
    "\n",
    "Remove the battery retainer, the optical flow module, and the pins attached to your board. The camera module will act as the battery holder and also keep the optical flow module attached. **Carefully** push the camera module where the battery retainer was until the pins stick out of the botton of the main body, ensure that it is facing foward on the drone and that there is enough room for the battery to slip between the main board and the camera module. Replace the optical flow board on these pins and position it as it was before. A picture is provided in `Lab7-photos`.\n",
    "\n",
    "Next, we can use cv2 to recieve the video from the camera and view it on this computer. Begin by pluggin your receiver into this computer and then run the following cell. If the cell reports an error, ensure that opencv-python is properly installed (see above). After installing, restart the kernel.\n",
    "\n",
    "A window which is probably showing static should appear. Next, we need to tune the receiver and camera module so they are on the same channel. The two buttons on the receiver change the received frequency down (left button) or up (right button) by 2 MHz. If you hold down the up button for a few seconds, the receiver will scan through all possibly frequencies, produce a histogram of the signal vs. frequency, and set the receiver to the frequency with the highest signal. Please note that to use this feature, you must ensure there are no other camera modudules nearby.\n",
    "\n",
    "Channels on the camera are identified by a `BAND` (blue LED, 1 on the left and 5 on the right) and `CH` (red LED, 1 on the left and 8 on the right), e.g. `BAND1 and CH1` for the two leftmost blue and red LEDs. You can change the `CH` number by pressing the button on the camera and the `BAND` by pressing the button for at least 2 seconds and then releasing. See the chart below for the list of frequencies corresponding to a particular BAND and CH frequency in MHz.\n",
    "\n",
    "| Channel | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 |\n",
    "| --- | --- | --- | --- | --- | --- | --- | --- | --- |\n",
    "| Band 1   | 5865 | 5845 | 5825 | 5805 | 5785 | 5765 | 5745 | 5725 | \n",
    "| Band 2   | 5733 | 5752 | 5771 | 5790 | 5809 | 5828 | 5847 | 5866 |\n",
    "| Band 3   | 5705 | 5685 | 5665 | 5665 | 5885 | 5905 | 5905 | 5905 |\n",
    "| Band 4   | 5740 | 5760 | 5780 | 5800 | 5820 | 5840 | 5860 | 5880 | \n",
    "| Band 5   | 5658 | 5695 | 5732 | 5769 | 5806 | 5843 | 5880 | 5917 |\n",
    "\n",
    "You will need to set the camera and receiver to operate on the same channel. We want to avoid intereference in the lab spaces when testing on the drone. The receiver has a large enough range that any CH and BAND can be used on the camera and the receiver will be able to pick up on the signal. However, the camera broadcasts strongest in the first 2 BANDS. Please set your camera `CH` to be the remainder of your group number when divided by 8. (i.e. if you're group number 15, you should set the `CH` 7) on Band 1 for groups 1-8, Band 2 for groups 9-16, and Band 3 for groups 17 on. This should avoid most interference but if there are issues in the lab space, you can pick another channel to tune to.\n",
    "\n",
    "Once you are done tuning, you can press 'q' to kill the window. Note that you may have to restart the kernel by going to the 'Kernel' and clicking 'Restart' as the cv2 window sometimes causes issues with jupyter-notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "# This may open your webcam instead of the CrazyFlie camera! If so, try\n",
    "# a different small, positive integer, e.g. 1, 2, 3.\n",
    "camera = 0\n",
    "cap = cv2.VideoCapture(camera)\n",
    "\n",
    "while(True):\n",
    "    # Capture frame-by-frame\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "\n",
    "    # Display the resulting frame\n",
    "    cv2.imshow('frame', frame)\n",
    "\n",
    "    # Hit q to quit.\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the capture\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instructions for recording video\n",
    "\n",
    "Next, you will record a video with the camera module so you can perform optical flow on your very own video. The following cell will connect to the camera module on the crazyflie and record a 5-second video. The cell will tell you when the video recording starts and ends. You must manually move the crazyflie during the recording. **Be sure to include a variety of movements, including: tilting up and down, and moving forwards. We have provided a video called \"optical_flow_test.avi\" in the github repository that demonstrates the kinds of movements you should make.** \n",
    "\n",
    "The result is a video titled `video_crazyflie.avi`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Currently \n",
    "\n",
    "import cv2\n",
    "import time\n",
    "\n",
    "# Remember to change this to whichever small int corresponds to the receiver from the previous code segment.\n",
    "cap = cv2.VideoCapture(camera)  \n",
    "\n",
    "fps = 20\n",
    "dt = 1 / fps\n",
    "image_size = (640, 480)\n",
    "file_name = 'video_crazyflie.avi'\n",
    "\n",
    "video = cv2.VideoWriter(file_name,cv2.VideoWriter_fourcc('M','J','P','G'), fps, image_size)\n",
    "\n",
    "print('Recording video!')\n",
    "for frame in range(1, fps * 5):  # record for 5 seconds\n",
    "    # Capture frame-by-frame\n",
    "    ret, frame = cap.read()\n",
    "    video.write(frame)\n",
    "    time.sleep(1 / fps)\n",
    "\n",
    "print('Recording finished!')\n",
    "# Release the capture\n",
    "cap.release()\n",
    "video.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's load in your video and visualize it. The following cell includes helper functions for visualization. **Do not edit this cell!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import time\n",
    "from IPython.display import HTML, Image\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import animation\n",
    "from typing import List, Tuple\n",
    "\n",
    "def load_video(name):\n",
    "    frames = []\n",
    "    cap = cv2.VideoCapture(name)\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        if ret == False:\n",
    "            break\n",
    "        else:\n",
    "            frames.append(frame)\n",
    "\n",
    "    cap.release()\n",
    "\n",
    "    I = np.zeros([image_size[1], image_size[0], len(frames)])\n",
    "    for t in range(len(frames)):\n",
    "        I[:, :, t] = cv2.cvtColor(cv2.resize(frames[t], image_size), cv2.COLOR_BGR2GRAY) / 255\n",
    "        \n",
    "    return I, fps\n",
    "    \n",
    "def create_gif(I, frame_rate=20):\n",
    "    fig, ax = plt.subplots()\n",
    "    im = ax.imshow(I[:, :, 0], cmap='gray')\n",
    "\n",
    "    def init():\n",
    "        return [im]\n",
    "\n",
    "    def animate(i):\n",
    "        ax.clear()\n",
    "        im = ax.imshow(I[:, :, i], cmap='gray')\n",
    "\n",
    "        return [im]\n",
    "\n",
    "    anim = animation.FuncAnimation(fig, animate, init_func=init, frames=I.shape[2], interval=100, blit=True)\n",
    "\n",
    "    anim.save('./anim.gif', writer='pillow', fps=frame_rate)\n",
    "    Image(url='./anim.gif')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variable `I` contains a grayscale video represented as a numpy array. How videos are represented in numpy is as a three dimensional array. If you want to access the pixel at $(x, y)$ at time $t$, it is stored at `I[y, x, t]`. Moreover, all values in `I` are between 0 (black) and 1 (white).\n",
    "\n",
    "The next cell creates a `.gif` file of the video so we can take a look at the video in motion. If the video isn't playing in jupyter-notebook, it is also saved as a `.gif` in the same directory as this notebook. Please ensure that the `.gif` file plays correctly on your computer (for Mac/Linux, the file may appear as a sequence of images). **Again, do not edit this cell!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline \n",
    "\n",
    "plt.rcParams['animation.ffmpeg_path'] = '/usr/bin/ffmpeg'\n",
    "print('Loading video!')\n",
    "I, frame_rate = load_video(file_name)\n",
    "\n",
    "print('Making GIF!')\n",
    "create_gif(I, frame_rate)\n",
    "print('GIF made!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can start processing the images. Your job below is to implement the optical flow algorithm discussed in class. For a single point, $(x, y)$, the window size $w$ is the side length of the box around the pixel. That is define $h = \\mathrm{floor}(w / 2)$, then the window is the set of pixels $\\{(x', y')\\ |\\ x - h \\leq x' \\leq x + h, y - h\\leq 'y \\leq y + h\\}$.\n",
    "\n",
    "You'll also need to take a number of image derivatives. [There are a lot of ways](https://en.wikipedia.org/wiki/Image_derivatives) one can go about taking them. We'll keep it simple, however, and say that the spatial derivatives $I_x, I_y$ at $(x, y)$ can be computed by subtracting the value at $(x, y)$ from $(x + 1, y + 1)$. Please be sure to note the format of the images (written in the docstring of the function template, below), to make sure that you assign $I_x$ and $I_y$ correctly! For the temporal derivative $I_t$, you can subtract the previous image from the current one.\n",
    "\n",
    "Using the algorithm as described so far, we'll run into trouble around the border of the image, as there are not enough pixels to create a window or potentially even take a derivative. To handle these cases, we will do the following:\n",
    "\n",
    "1. Take the image derivatives first. Throw out all pixels for which they are not defined.\n",
    "\n",
    "2. Create as large a window as possible around the target pixel $(x, y)$. If there are not enough pixels for a complete window, use all the pixels available in the region.\n",
    "\n",
    "Also, in class, it was discussed that there are cases where the matrix:\n",
    "\n",
    "$$A^{\\mathrm{T}}A = \\begin{bmatrix}\\sum I_x I_x & \\sum I_x I_y\\\\ \\sum I_y I_x & \\sum I_y I_y\\end{bmatrix}$$\n",
    "\n",
    "is not invertible. In practice, the determinant of $A^{\\mathrm{T}}A$ is unlikely to be exactly zero due to the numerical approximations the computer makes. It may, however, be very small. Since we are solving for two unknowns $u, v$, we need the rank of this matrix to be at least 2. Therefore, we need at least two of the eigenvalues of $A^{\\mathrm{T}}A$ to be of a reasonable size. After computing $A$, you should only solve the least-squares problem for $u, v$ if at least two eigenvalues are larger than $0.001$. Otherwise just set $u, v$ to be zero for the point at which you are evaluating the optical flow. You can compute the eigenvalues with Numpy's [eigvals](https://docs.scipy.org/doc/numpy/reference/generated/numpy.linalg.eigvals.html) function.\n",
    "\n",
    "Be sure you are using the correct coordinates when accessing the image. It is a common pitfall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optical_flow(prev_image: np.ndarray, image: np.ndarray, pts: np.ndarray, win_size: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Compute the optical flow between two images using the Lucas-Kanade algorithm.\n",
    "    \n",
    "    @param prev_image: The image at time t - 1. Images are represented as arrays of the form I[y, x, t]\n",
    "                       and are grayscale.\n",
    "    @param image: The image at time t.\n",
    "    @param pts: An n-by-2 array of points at which to evaluate the optical flow. Each point is\n",
    "                 of the form [x, y].\n",
    "    @param win_size: The size of the window used to compute the flow.\n",
    "    \n",
    "    @return: An n-by-2 array whose ith row contains the optical flow at the ith row in pts.\n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets apply it to the image. This may take some time to run. In practice, optical flow can be computed extremely quickly, but our code is slow for pedagogical purposes. If you want to just test your code quickly, change the value of `horizon` to some small value like 10. Be sure to change it back before submission, though. You can also play with the `window_size` variable to see how it impacts the computed flow. Please submit your notebook with `window_size = 30` (the default)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 30\n",
    "\n",
    "def optical_flow_video(I):\n",
    "    horizon = I.shape[2]\n",
    "\n",
    "    # Compute the optical flow at each point.\n",
    "    flow = np.zeros((I.shape[0] - 1, I.shape[1] - 1, I.shape[2] - 1, 2))\n",
    "\n",
    "    for t in range(1, horizon):\n",
    "        if t % 10 == 0:\n",
    "            print('Completed {0} frames.'.format(t))\n",
    "\n",
    "        for j in range(0, flow.shape[0], 25):\n",
    "            for i in range(0, flow.shape[1], 25):\n",
    "                flow[j, i, t - 1] = optical_flow(I[:, :, t - 1], I[:, :, t], np.array([[i, j]]).astype(float), window_size)\n",
    "\n",
    "    mean_flow = np.zeros((flow.shape[2], 2))\n",
    "\n",
    "    # Compute the mean flow at each time step.\n",
    "    for t in range(mean_flow.shape[0]):\n",
    "        mean_flow[t, 0] = flow[:, :, t, 0].mean()\n",
    "        mean_flow[t, 1] = flow[:, :, t, 1].mean()\n",
    "\n",
    "    plt.plot(np.arange(0, mean_flow.shape[0])*dt, mean_flow[:, 0], label='X Mean')\n",
    "    plt.plot(np.arange(0, mean_flow.shape[0])*dt, mean_flow[:, 1], label='Y Mean')\n",
    "    plt.legend()\n",
    "    return flow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell runs the optical flow defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flow = optical_flow_video(I)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we'll plot some of the optical flow values on top of the video itself. The next cell includes helper functions for visualizing the result from optical flow. **Do not edit this cell!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.cm as cm\n",
    "from matplotlib.colors import Normalize\n",
    "\n",
    "def visualize_optical_flow(flow, I, frame_rate):\n",
    "    horizon = I.shape[2]\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    cI = np.zeros((I.shape[0], I.shape[1], 3, I.shape[2]))\n",
    "\n",
    "    for t in range(I.shape[2]):\n",
    "        cI[:, :, :, t] = cv2.cvtColor((I[:, :, t] * 255).astype(np.uint8), cv2.COLOR_GRAY2RGB).astype(float) / 255\n",
    "\n",
    "    im = ax.imshow(cI[:, :, :, 0])\n",
    "\n",
    "    num_arrows = len(range(0, flow.shape[0], 25)) * len(range(0, flow.shape[1], 25))\n",
    "\n",
    "    cx = np.zeros(num_arrows)\n",
    "    cy = np.zeros(num_arrows)\n",
    "    u = np.zeros(num_arrows)\n",
    "    v = np.zeros(num_arrows)\n",
    "\n",
    "    k = 0\n",
    "\n",
    "    for j in range(0, flow.shape[0], 25):\n",
    "        for i in range(0, flow.shape[1], 25):\n",
    "            cy[k] = j\n",
    "            cx[k] = i\n",
    "            u[k] = 0.1\n",
    "            v[k] = 0.1\n",
    "            k += 1   \n",
    "    \n",
    "    quiv = ax.quiver(cx, cy, u, v, width=.01)\n",
    "\n",
    "    def init():\n",
    "        return [im, quiv]\n",
    "\n",
    "    def animate(t, window_size):\n",
    "        boxes = []\n",
    "        y_windows = int(I.shape[0] / window_size)\n",
    "        x_windows = int(I.shape[1] / window_size)\n",
    "\n",
    "        cx = np.zeros(num_arrows)\n",
    "        cy = np.zeros(num_arrows)\n",
    "        u = np.zeros(num_arrows)\n",
    "        v = np.zeros(num_arrows)\n",
    "\n",
    "        k = 0\n",
    "\n",
    "        for j in range(0, flow.shape[0], 25):\n",
    "            for i in range(0, flow.shape[1], 25):\n",
    "                cy[k] = j\n",
    "                cx[k] = i\n",
    "\n",
    "                if np.linalg.norm(flow[j, i, t, :]) > 0.0001:\n",
    "                    u[k] = 0.1 * flow[j, i, t, 0] / np.linalg.norm(flow[j, i, t, :])\n",
    "                    v[k] = -0.1 * flow[j, i, t, 1] / np.linalg.norm(flow[j, i, t, :])\n",
    "                else:\n",
    "                    u[k] = 0\n",
    "                    v[k] = 0\n",
    "                k += 1\n",
    "        \n",
    "        ax.clear()\n",
    "        im = ax.imshow(cI[:, :, :, t])\n",
    "\n",
    "        colors = np.arctan2(u, v)\n",
    "        norm = Normalize()\n",
    "        colormap = cm.hsv\n",
    "        \n",
    "        quiv = ax.quiver(cx, cy, u, v, color=colormap(norm(colors)), width=.01)\n",
    "\n",
    "        return [im, quiv]\n",
    "\n",
    "    anim = animation.FuncAnimation(fig, lambda t: animate(t, window_size), init_func=init, frames=(horizon - 2), interval=100, blit=True)\n",
    "\n",
    "    anim.save('./anim2.gif', writer='pillow', fps=frame_rate)\n",
    "    Image(url='./anim2.gif')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell will produce another `.gif` file which has the result from optical flow overlaid on top of your video. Again, if the video isn't playing in jupyter-notebook, it is also saved as a `.gif` in the same directory as this notebook. Please ensure that the `.gif` file plays correctly on your computer.\n",
    "\n",
    "If you implemented things correctly, there should be a few trends you can observe in the video:\n",
    "1. Arrows should generally be pointing down when the camera rotates up and pointing up when the camera rotates down.\n",
    "\n",
    "2. When the camera moves forward (what is known as a \"dolly in\" motion), the arrows should generally be pointing radially outward (for example, arrows along the bottom of the image should be pointed downward). When the camera moves back (\"dolly out\"), the arrows should generally move radially inward.\n",
    "\n",
    "3. Regions of the image without texture may have random arrows, as there are no trackable flow features in those regions.\n",
    "\n",
    "These phenomena may not appear perfectly. Since the video was filmed by hand, there is some amount of shakiness that contributes to the optical flow. You can also play around with the window size to see whether or not there is improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_optical_flow(flow, I, frame_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission Instructions\n",
    "\n",
    "As usual, you should submit your completed notebook, `anim.gif`, and `anim2.gif` to HW7: Coding on Gradescope.\n",
    "\n",
    "**Be sure to submit _both_ video files with your notebook!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
